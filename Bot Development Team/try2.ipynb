{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  D:\\\\LLM_models\\\\TheBloke\\\\Llama-2-13B-chat-GGML\\\\llama-2-13b-chat.ggmlv3.q4_1.bin\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Let's think step by step of the question: Tell me about Langchain\n",
      "Based on all the thought the final answer becomes:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Tell me about Langchain',\n",
       " 'text': 'Langchain is a blockchain that can be used to store information. It has been designed with security in mind, and it provides an additional layer of protection for your data. With this feature enabled, you will have peace of mind knowing that your data is safe from unauthorized access.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# create a prompt template where it contains some initial instructions\n",
    "# here we say our LLM to think step by step and give the answer\n",
    "\n",
    "template = \"\"\"\n",
    "Let's think step by step of the question: {question}\n",
    "Based on all the thought the final answer becomes:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# paste the path where your model's weight are located (.bin file)\n",
    "# you can download the models by going to gpt4all's website.\n",
    "# scripts for downloading is also available in the later \n",
    "\n",
    "local_path = (\"D:\\LLM_models\\TheBloke\\Llama-2-13B-chat-GGML\\llama-2-13b-chat.ggmlv3.q4_1.bin\")\n",
    "\n",
    "# initialize the LLM and make chain it with the prompts\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=local_path, \n",
    "    # backend=\"llama\", \n",
    "    max_tokens = -1,\n",
    "    top_p=  0.9,\n",
    "    temp = 0.2,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "# run the chain with your query (question)\n",
    "\n",
    "llm_chain('Tell me about Langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Let's think step by step of the question: Tell me about AI\n",
      "Based on all the thought the final answer becomes:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"The only thing that counts is the number of steps it takes to get an answer.\"'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain('Tell me about AI')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Let's think step by step of the question: Tell me about Langchain\n",
      "Based on all the thought the final answer becomes:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Langchain is a blockchain-based platform that enables decentralized finance.\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain('Tell me about Langchain')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  D:\\\\LLM_models\\\\TheBloke\\\\Llama-2-13B-chat-GGML\\\\llama-2-13b-chat.ggmlv3.q4_1.bin\n",
      "?\n",
      "The word \"Lang\" can have different meanings depending on the Lang. For example, in Germanic mythology, \"Lang\" refers to a specific type of tree that has been used for centuries as an important part of Norse culture and tradition. In this context, \"Lang\" is not just another name for Odin but also represents Thor's hammer - both symbols representing strength through adversity rather than weakness due solely relying on brute force"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"D:\\LLM_models\\TheBloke\\Llama-2-13B-chat-GGML\\llama-2-13b-chat.ggmlv3.q4_1.bin\")\n",
    "tokens = []\n",
    "for token in model.generate(\"What is Langchain\", max_tokens=100, streaming=True):\n",
    "    print(token, end=\"\")\n",
    "    tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  D:\\\\LLM_models\\\\TheBloke\\\\Llama-2-13B-chat-GGML\\\\llama-2-13b-chat.ggmlv3.q4_1.bin\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Let's think step by step of the question: Who is the CEO of Google and why he became the ceo of Google?\n",
      "Based on all the thought the final answer becomes:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'Who is the CEO of Google and why he became the ceo of Google?', 'text': 'The CEO of Google is Sundar Pichai.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Let's think step by step of the question: {question}\n",
    "Based on all the thought the final answer becomes:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "\n",
    "local_path = (\"D:\\LLM_models\\TheBloke\\Llama-2-13B-chat-GGML\\llama-2-13b-chat.ggmlv3.q4_1.bin\")\n",
    "\n",
    "# initialize the LLM and make chain it with the prompts\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=local_path, \n",
    "    backend=\"llama\", \n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "# run the chain with your query (question)\n",
    "\n",
    "llm_chain('Who is the CEO of Google and why he became the ceo of Google?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp_python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Chatbot\\gpt\\gpt\\Agents\\try2.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Chatbot/gpt/gpt/Agents/try2.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mllama_cpp_python\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Chatbot/gpt/gpt/Agents/try2.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m llm \u001b[39m=\u001b[39m llama_cpp_python\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLLM_models\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTheBloke\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLlama-2-13B-chat-GGML\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mllama-2-13b-chat.ggmlv3.q4_1.bin\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp_python'"
     ]
    }
   ],
   "source": [
    "import llama_cpp_python\n",
    "llm = llama_cpp_python.load_model(\"D:\\LLM_models\\TheBloke\\Llama-2-13B-chat-GGML\\llama-2-13b-chat.ggmlv3.q4_1.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatproj",
   "language": "python",
   "name": "chatproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
